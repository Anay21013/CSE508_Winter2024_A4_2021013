# -*- coding: utf-8 -*-
"""CSE508_Winter2024_A4_2021013.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cteNxWwVvZoH_stegTx_p-JCnVCBbAy_
"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

# Download stopwords if not already downloaded
nltk.download('stopwords')

# Load data
df = pd.read_csv('F:\Programming Stuff\IR\Reviews.csv')

# Select 1200 random rows
df_sample = df.sample(n=1200, random_state=42).copy()

# Remove duplicates
df_sample.drop_duplicates(inplace=True)

# Handle missing values
df_sample.dropna(subset=['Text', 'Summary'], inplace=True)

def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    # Remove non-alphabetic characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text)
    # Remove leading and trailing whitespaces
    text = text.strip()
    return text

def remove_stopwords(text):
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = text.split()
    filtered_tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(filtered_tokens)

# Apply cleaning functions to 'Text' and 'Summary' columns
df_sample['Text'] = df_sample['Text'].apply(clean_text).apply(remove_stopwords)
df_sample['Summary'] = df_sample['Summary'].apply(clean_text).apply(remove_stopwords)

print(df_sample['Text'])
print(df_sample['Summary'])

import torch
print(torch.cuda.is_available())
print(torch.cuda.device_count())
device = torch.device("cuda is available!" if torch.cuda.is_available() else "cpu is being used!")
print(device)

from transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers import GPT2Tokenizer

# Initialize the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Initialize the model
model = GPT2LMHeadModel.from_pretrained('gpt2')
model = model.to(device)

from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets
train_data, test_data = train_test_split(df_sample, test_size=0.25, random_state=42)

# Add TL;DR token to designate the summarization task
train_data['Input_Text'] = train_data['Text'] + " TL;DR " + train_data['Summary']

# Create a review array with "Text + TL;DR + Summary" for the training dataset only
reviews_array_train = train_data['Input_Text'].tolist()
print(reviews_array_train)

test_data['Test_Text'] = test_data['Text'] + " TL;DR"
reviews_array_test = test_data['Test_Text'].tolist()
print(reviews_array_test)

torch.cuda.empty_cache()

from torch.utils.data import Dataset
import torch

class ReviewDataset(Dataset):
    def __init__(self, reviews, tokenizer, max_length):
        self.reviews = reviews
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

    def __len__(self):
        return len(self.reviews)

    def __getitem__(self, idx):
        review = self.reviews[idx]

        # Tokenize the combined text
        tokens = self.tokenizer.encode(review, add_special_tokens=True)

        # Truncate or pad tokens to max_length
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]
        else:
            padding_length = self.max_length - len(tokens)
            tokens = tokens + [self.tokenizer.pad_token_id] * padding_length

        # Prepare labels (shifted by one position) for LM training
        labels = tokens[1:] + [self.tokenizer.pad_token_id]

        return {
            'input_ids': torch.tensor(tokens),
            'labels': torch.tensor(labels)
        }

from transformers import Trainer, TrainingArguments
from torch.utils.data import DataLoader

# Define the training arguments
training_args = TrainingArguments(
    output_dir='./results',                  # output directory
    num_train_epochs=3,                      # number of training epochs
    per_device_train_batch_size=16,          # batch size for training
    logging_dir='./logs',                    # directory for storing logs
    logging_steps=100,                       # log every 100 steps
    save_steps=100,                          # save checkpoint every 100 steps
)

# Create the DataLoader
train_dataset = ReviewDataset(reviews_array_train, tokenizer, max_length=100)
train_loader = DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size, shuffle=True)

# Define the Trainer for fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    train_loader=train_loader,
)

# Fine-tune the model
trainer.train()

model_path = "./results"
model.save_pretrained(model_path)

import torch

def evaluate_model(model, tokenizer, test_data, max_length=200):
    """
    Evaluate the given model on the provided test data.

    Args:
    - model: Pre-trained GPT2LMHeadModel.
    - tokenizer: GPT2Tokenizer used for tokenizing input data.
    - test_data: DataFrame containing test data.
    - max_length: Maximum length for generated text.
    """
    model.eval()
    with torch.no_grad():
        for index, row in test_data.iterrows():
            input_ids = row['input_ids'].unsqueeze(0)  # Unsqueeze to add batch dimension
            generated_ids = model.generate(
                input_ids=input_ids,
                max_length=len(input_ids[0]) + 30,  # Set the maximum length of the generated text
                num_beams=4,     # Set the number of beams for beam search
                length_penalty=2.0,  # Set the length penalty for beam search
                repetition_penalty=2.0,  # Set the repetition penalty for beam search
                pad_token_id=tokenizer.pad_token_id,  # Set the pad token ID
                eos_token_id=tokenizer.eos_token_id,  # Set the end-of-sequence token ID
                early_stopping=False  # Enable early stopping
            )
            # Decode the generated text
            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            print("Generated Text:", generated_text)

# Assuming you have the necessary imports and definitions already done

# Load the saved model
model_path = "./results/checkpoint-100"
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained(model_path)

# Assuming you have a ReviewDataset class for the test data
test_dataset = ReviewDataset(reviews_array_test, tokenizer, max_length=200)

# Call the evaluate_model function
evaluate_model(model, tokenizer, test_data)

